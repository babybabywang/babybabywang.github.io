<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[SpringCloud基本概念以及服务注册与服务调用]]></title>
    <url>%2F2019%2F01%2F27%2FSpringCloud%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%BB%A5%E5%8F%8A%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E4%B8%8E%E6%9C%8D%E5%8A%A1%E8%B0%83%E7%94%A8%2F</url>
    <content type="text"><![CDATA[SpringCloud简介什么是SpringCloud? Spring Cloud是一系列框架的有序集合。它利用Spring Boot的开发便利性巧妙地简化了分布式系统基础设施的开发，如服务发现注册、配置中心、消息总线、负载均衡、熔断器、数据监控等，都可以用Spring Boot的开发风格做到一键启动和部署。Spring并没有重复制造轮子，它只是将目前各家公司开发的比较成熟、经得起实际考验的服务框架组合起来，通过Spring Boot风格进行再封装屏蔽掉了复杂的配置和实现原理，最终给开发者留出了一套简单易懂、易部署和易维护的分布式系统开发工具包。 &nbsp;&nbsp;&nbsp;Spring Cloud项目的官方网址 SpringCloud与SpringBoot的关系 &nbsp;&nbsp;&nbsp;Spring Boot 是 Spring 的一套快速配置脚手架，可以基于Spring Boot 快速开发单个微服务，Spring Cloud是一个基于Spring Boot实现的云应用开发工具；Spring Boot专注于快速、方便集成的单个微服务个体，Spring Cloud关注全局的服务治理框架；Spring Boot使用了默认大于配置的理念，很多集成方案已经帮你选择好了，能不配置就不配置，Spring Cloud很大的一部分是基于Spring Boot来实现，可以不基于Spring Boot吗？不可以。 Spring Boot可以离开Spring Cloud独立使用开发项目，但是Spring Cloud离不开Spring Boot，属于依赖的关系。 Spring Cloud和Dubbo对比 或许很多人会说Spring Cloud和Dubbo的对比有点不公平，Dubbo只是实现了服务治理，而Spring Cloud下面有17个子项目（可能还会新增）分别覆盖了微服务架构下的方方面面，服务治理只是其中的一个方面，一定程度来说，Dubbo只是Spring CloudNetflix中的一个子集。 这里可以去了解下我以前一篇文章了解下RPC和HTTP直接的区别，也可以帮助你了解Dubbo和SC之间的区别简述HTTP和RPC的优缺点 服务发现组件 EurekaEureka Eureka是Netflix开发的服务发现框架，SpringCloud将它集成在自己的子项目 spring-cloud-netflix中，实现SpringCloud的服务发现功能。Eureka包含两个组件： Eureka Server和Eureka Client。 Eureka Server提供服务注册服务，各个节点启动后，会在Eureka Server中进行注 册，这样EurekaServer中的服务注册表中将会存储所有可用服务节点的信息，服务节点 的信息可以在界面中直观的看到。 Eureka Client是一个java客户端，用于简化与Eureka Server的交互，客户端同时也 就别一个内置的、使用轮询(round-robin)负载算法的负载均衡器。在应用启动后，将会 向Eureka Server发送心跳,默认周期为30秒，如果Eureka Server在多个心跳周期内没有 接收到某个节点的心跳，Eureka Server将会从服务注册表中把这个服务节点移除(默认90 秒)。 Eureka Server之间通过复制的方式完成数据的同步，Eureka还提供了客户端缓存机 制，即使所有的Eureka Server都挂掉，客户端依然可以利用缓存中的信息消费其他服务 的API。综上，Eureka通过心跳检查、客户端缓存等机制，确保了系统的高可用性、灵活 性和可伸缩性。 Eureka服务端开发 创建注册中心模块 引入依赖 &lt;properties&gt; &lt;spring-cloud.version&gt; Finchley.RELEASE&lt;/spring-cloud.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;${spring-cloud.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; 添加application.yml配置 server: port: 9010 spring: application: name: eureka-register eureka: client: service-url: defaultZone: http://localhost:${server.port}/eureka/ register-with-eureka: false fetch-registry: false 在启动类加上@EnableEurekaServer 服务注册 引入依赖 &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; 编写application.yml文件 eureka: client: service-url: defaultZone: http://localhost:9010/eureka/ instance: prefer-ip-address: true instance-id: ${spring.application.name}:${spring.application.instance_id:${server.port}} 注意F版本后，客户端无需在启动类加@EnableEurekaClient注解 ### 保护模式 如果在Eureka Server的首页看到以下这段提示，则说明Eureka已经进入了保护模式： &nbsp;&nbsp;&nbsp;Eureka Server在运行期间，会统计心跳失败的比例在15分钟之内是否低于85%，如果出现低于的情况（在单机调试的时候很容易满足，实际在生产环境上通常是由于网络不稳定导致），Eureka Server会将当前的实例注册信息保护起来，同时提示这个警告。保护模式主要用于一组客户端和Eureka Server之间存在网络分区场景下的保护。一旦进入保护模式，Eureka Server将会尝试保护其服务注册表中的信息，不再删除服务注册表中的数据（也就是不会注销任何微服务）。 Feign实现服务间的调用Feign简介 &nbsp;&nbsp;&nbsp;Feign是简化Java HTTP客户端开发的工具（java-to-httpclient-binder），它的灵感来自于Retrofit、JAXRS-2.0和WebSocket。Feign的初衷是降低统一绑定Denominator到HTTP API的复杂度，不区分是否为restful. (通过Feign)实现问答服务调用基础服务 引入依赖 &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;/dependency&gt; 在启动类添加@EnableFeignClients和@EnableDiscoveryClient 调用Base服务接口 /** * 通过Feign调用服务 * @author huangsm */ @FeignClient(name = &quot;tensquare-base&quot;) public interface BaseClient { /** * 调用base服务的根据ID查询文章接口 * @param labelId * @return */ @GetMapping(&quot;/label/{labelId}&quot;) Result findById(@PathVariable(&quot;labelId&quot;) String labelId); } 远程服务的使用 @Autowired private BaseClient baseClient; @GetMapping(value = &quot;/label/{labelid}&quot;) public Result findLabelById(@PathVariable String labelid){ Result result = baseClient.findById(labelid); return result; } ### 负载均衡 因为Feign内集成了rabbion所以引入feign后服务带有负载均衡]]></content>
      <categories>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>SpringCloud</tag>
        <tag>Dubbo</tag>
        <tag>Eureka</tag>
        <tag>Feign</tag>
        <tag>rabbion</tag>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于JWT的微服务鉴权开发实现]]></title>
    <url>%2F2019%2F01%2F26%2F%E5%9F%BA%E4%BA%8EJWT%E7%9A%84%E5%BE%AE%E6%9C%8D%E5%8A%A1%E9%89%B4%E6%9D%83%E5%BC%80%E5%8F%91%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[鉴权微服务开发JWT工具类编写/** * JWT工具类 * @author huangsm */ @ConfigurationProperties(&quot;jwt.config&quot;) public class JwtUtil { private String key ; /** * 一个小时 */ private long ttl ; public String getKey() { return key; } public void setKey(String key) { this.key = key; } public long getTtl() { return ttl; } public void setTtl(long ttl) { this.ttl = ttl; } /** * 生成JWT * * @param id * @param subject * @return */ public String createJWT(String id, String subject, String roles) { long nowMillis = System.currentTimeMillis(); Date now = new Date(nowMillis); JwtBuilder builder = Jwts.builder().setId(id) .setSubject(subject) .setIssuedAt(now) .signWith(SignatureAlgorithm.HS256, key).claim(&quot;roles&quot;, roles); if (ttl &gt; 0) { builder.setExpiration( new Date( nowMillis + ttl)); } return builder.compact(); } /** * 解析JWT * @param jwtStr * @return */ public Claims parseJWT(String jwtStr){ return Jwts.parser() .setSigningKey(key) .parseClaimsJws(jwtStr) .getBody(); } } 管理员登陆后台签发token１． 修改全局配置文件，配置token的盐和过期时间 jwt: config: key: huangsmzhenshuai ttl: 3600000 将Jwt工具类放入Spring容器 @Bean public JwtUtil jwtUtil(){ return new JwtUtil(); } 管理员登录后台token签发 1.控制层代码: @PostMapping(value = &quot;/login&quot;) public Result login(@RequestBody Admin admin) { String token = adminService.login(admin.getLoginname(), admin.getPassword()); Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); map.put(&quot;token&quot;, token); map.put(&quot;roles&quot;, &quot;admin&quot;); map.put(&quot;loginName&quot;, admin.getLoginname()); return new Result(true, StatusCode.OK, &quot;登录成功&quot;, map); } 2.服务层代码: @Autowired private JwtUtil jwtUtil; public String login(String loginName,String password){ Admin sysAdmin = adminDao.findByLoginname(loginName); if (sysAdmin==null){ throw new RuntimeException(&quot;用户名不存在!&quot;); } if (!bCryptPasswordEncoder.matches(password,sysAdmin.getPassword())){ throw new RuntimeException(&quot;密码错误!&quot;); } String token = jwtUtil.createJWT(loginName, password, &quot;admin&quot;); return token; } 管理员删除用户功能鉴权 需求：删除用户，必须拥有管理员权限，否则不能删除 前后端约定：前端请求微服务时需要添加头信息Authorization ,内容为Bearer+空格+token 业务代码: /** * 删除必须有admin才可以删除 * * @param id */ public void deleteById(String id) { String authorization = request.getHeader(&quot;Authorization&quot;); if (StringUtils.isEmpty(authorization)){ throw new RuntimeException(&quot;权限不足!&quot;); } if (!authorization.startsWith(&quot;Bearer&quot;)){ throw new RuntimeException(&quot;权限不足!&quot;); } //得到token String token = authorization.substring(7); try { Claims claims = jwtUtil.parseJWT(token); if (!claims.get(&quot;roles&quot;).equals(&quot;admin&quot;)||claims.get(&quot;roles&quot;)==null){ throw new RuntimeException(&quot;权限不足!&quot;); } }catch (Exception e){ throw new RuntimeException(&quot;权限不足!&quot;); } userDao.deleteById(id); } 上述代码你会发现个问题，每次验证都需要写一大堆业务，代码过于重复，这里使用拦截器来完成鉴权 一、编写拦截器 /** * 授权拦截器 * * @author huangsm */ @Component public class JwtInterceptor implements HandlerInterceptor { /** * Spring为我们提供了org.springframework.web.servlet.handler.HandlerInterceptorAdapter这个适配器， * 继承此类，可以非常方便的实现自己的拦截器。 * 他有三个方法：分别实现预处理、后处理（调用了Service并返回ModelAndView，但未进行页面渲染）、返回处理（已经渲染了页面） * 在preHandle中，可以进行编码、安全控制等处理； * 在postHandle中，有机会修改ModelAndView； * 在afterCompletion中，可以根据ex是否为null判断是否发生了异常，进行日志记 */ @Autowired private JwtUtil jwtUtil; /** * 前置拦截器 * * @param request * @param response * @param handler * @return * @throws Exception */ @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { System.out.println(&quot;经过了拦截器&quot;); //无论如何都放下。具体能不能操作还是在具体操作中去判断 //拦截器只是负责把请求头中包含token的令牌进行解析验证。 final String authorization = request.getHeader(&quot;Authorization&quot;); //如果包含有Authorization头信息，就对其进行判断 if (authorization != null &amp;&amp; authorization.startsWith(&quot;Bearer&quot;)) { //得到token final String token = authorization.substring(7); try { Claims claims = jwtUtil.parseJWT(token); if (claims != null) { String roles = (String) claims.get(&quot;roles&quot;); //如果是管理员 if (&quot;admin&quot;.equals(roles) &amp;&amp; roles != null) { request.setAttribute(&quot;admin_claims&quot;, claims); } //如果是用户 if (&quot;user&quot;.equals(roles) &amp;&amp; roles != null) { request.setAttribute(&quot;user_claims&quot;, claims); } } } catch (Exception e) { throw new RuntimeException(&quot;令牌有误!&quot;); } } return true; } } 二、将拦截器加入到SpringMvc中 /** * Web配置类 * * @author huangsm */ @Configuration public class WebConfig extends WebMvcConfigurationSupport { @Autowired private JwtInterceptor jwtInterceptor; /** * 添加拦截器 * addPathPatterns拦截的路径 * excludePathPatterns不拦截的路径 * @param registry */ @Override public void addInterceptors(InterceptorRegistry registry) { //注册拦截器要声明拦截器对象和要拦截的请求 registry.addInterceptor(jwtInterceptor) .addPathPatterns(&quot;/**&quot;) .excludePathPatterns(&quot;/**/login&quot;); } } 三、修改原有的删除用户逻辑 /** * 删除必须有admin才可以删除 * * @param id */ public void deleteById(String id) { Claims admin_claims = (Claims) request.getAttribute(&quot;admin_claims&quot;); if (admin_claims == null) { throw new RuntimeException(&quot;权限不足!&quot;); } userDao.deleteById(id); } 这是是一个拦截器的简单使用，其实可以加上角色认证和aop注解栏控制那些请求不需要拦截]]></content>
      <categories>
        <category>JWT</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>jwt</tag>
        <tag>权限拦截器</tag>
        <tag>鉴权微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于JWT的Token认证机制以及实现]]></title>
    <url>%2F2019%2F01%2F26%2F%E5%9F%BA%E4%BA%8EJWT%E7%9A%84Token%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6%E4%BB%A5%E5%8F%8A%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[前提要求考虑到使用SpringSecurity的BCryptPasswordEncoder盐加密算法所以在服务中引入SpringSecurity 引入依赖 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt; &lt;/dependency&gt; 我们在添加了spring security依赖后，所有的地址都被spring security所控制了，我们目前只是需要用到BCrypt密码加密的部分，所以我们要添加一个配置类，配置为所有地址都可以匿名访问 /** * 安全配置类 * @author huangsm */ @EnableWebSecurity @Configuration public class WebSecurityConfig extends WebSecurityConfigurerAdapter { /** * authorizeRequests所有security全注解配置实现的开端，表示开始说明需要的权限。 * 需要的权限分两部分，第一部分是拦截的路径，第二部分访问该路径需要的权限。 * antMatchers表示拦截什么路径，permitAll任何权限都可以访问，直接放行所有。 * anyRequest()任何的请求，authenticated认证后才能访问 * and().csrf().disable()；固定写法，标识csrf失效。 * @param http * @throws Exception */ @Override protected void configure(HttpSecurity http) throws Exception { http .authorizeRequests() .antMatchers(&quot;/**&quot;).permitAll() .anyRequest().authenticated() .and().csrf().disable(); } /** * 配置BCrypt强哈希方法 每次加密的结果都不一样 * @return */ @Bean public BCryptPasswordEncoder bCryptPasswordEncoder(){ return new BCryptPasswordEncoder(); } } 基于JWT的Token认证机制关于有状态登录和无状态的登录 有状态登录: 服务器端需要存储用户信息 无状态登录: 服务器端不需要存储用户信息 常见的认证机制 HTTP Basic Auth（无状态登录） HTTP Basic Auth简单点说明就是每次请求API时都提供用户的username和password，简言之，Basic Auth是配合RESTful API 使用的最简单的认证方式，只需提供用户名密码即可，但由于有把用户名密码暴露给第三方客户端的风险，在生产 环境下被使用的越来越少。因此，在开发对外开放的RESTful API时，尽量避免采用HTTP BasicAuth Cookie Auth Cookie认证机制就是为一次请求认证在服务端创建一个Session对象，同时在客户端的浏览器端创建了一个 Cookie对象；通过客户端带上来Cookie对象来与服务器端的session对象匹配来实现状态管理的。默认的，当我 们关闭浏览器的时候，cookie会被删除。但可以通过修改cookie 的expire time使cookie在一定时间内有效。 OAuth OAuth（开放授权）是一个开放的授权标准，允许用户让第三方应用访问该用户在 某一web服务上存储的私密的资源（如照片，视频，联系人列表），而无需将用户名和 密码提供给第三方应用。 OAuth允许用户提供一个令牌，而不是用户名和密码来访问他们存放在特定服务提供者的数据。每一个令牌授权一 个特定的第三方系统（例如，视频编辑网站)在特定的时段（例如，接下来的2小时内）内访问特定的资源（例如 仅仅是某一相册中的视频）。这样，OAuth让用户可以授权第三方网站访问他们存储在另外服务提供者的某些特定 信息，而非所有 下面是OAuth2.0的流程: 这种基于OAuth的认证机制适用于个人消费者类的互联网产品，如社交类APP等应 用，但是不太适合拥有自有认证权限管理的企业应用。 Token Auth 使用基于 Token 的身份验证方法，在服务端不需要存储用户的登录记录。大概的流程是 这样的： 1.客户端使用用户名跟密码请求登录 2.服务端收到请求，去验证用户名与密码 3.验证成功后，服务端会签发一个 Token，再把这个 Token 发送给客户端 4.客户端收到 Token 以后可以把它存储起来，比如放在 Cookie 里 5.客户端每次向服务端请求资源的时候需要带着服务端签发的 Token 6.服务端收到请求，然后去验证客户端请求里面带着的 Token，如果验证成功，就向 客户端返回请求的数据 Token Auth的优点: Token机制相对于Cookie机制又有什么好处呢？ 支持跨域访问: Cookie是不允许垮域访问的，这一点对Token机制是不存在的，前提是传输的用户认证信息通过HTTP头传输. 无状态(也称：服务端可扩展行):Token机制在服务端不需要存储session信息，因为Token 自身包含了所有登录用户的信息，只需要在客户端的cookie或本地介质存储状态信息. 更适用CDN: 可以通过内容分发网络请求你服务端的所有资料（如：javascript，HTML,图片等），而你的服务端只要提供API即可. 去耦: 不需要绑定到一个特定的身份验证方案。Token可以在任何地方生成，只要在你的API被调用的时候，你可以进行Token生成调用即可.更适用于移动应用: 当你的客户端是一个原生平台（iOS, Android，Windows 8等）时，Cookie是不被支持的（你需要通过Cookie容器进行处理），这时采用Token认证机制就会简单得多。 CSRF:因为不再依赖于Cookie，所以你就不需要考虑对CSRF（跨站请求伪造）的防范。 性能: 一次网络往返时间（通过数据库查询session信息）总比做一次HMACSHA256计算 的Token验证和解析要费时得多. 不需要为登录页面做特殊处理: 如果你使用Protractor 做功能测试的时候，不再需要为登录页面做特殊处理. 基于标准化:你的API可以采用标准化的 JSON Web Token (JWT). 这个标准已经存在多个后端库（.NET, Ruby, Java,Python, PHP）和多家公司的支持（如：Firebase,Google, Microsoft）.基于JWT的Token认证机制实现 什么是JWT? JSON Web Token（JWT）是一个非常轻巧的规范。这个规范允许我们使用JWT在用户和服务器之间传递安全可靠的信息. JWT组成 一个JWT实际上就是一个字符串，它由三部分组成，头部、载荷与签名。 头部（Header） 头部用于描述关于该JWT的最基本的信息，例如其类型以及签名所用的算法等。这也可以被表示成一个JSON对象。 {&quot;typ&quot;:&quot;JWT&quot;,&quot;alg&quot;:&quot;HS256&quot;} 在头部指明了签名算法是HS256算法。 我们进行BASE64编码http://base64.xpcha.com/，编码后的字符串如下： eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1Ni 小知识：Base64是一种基于64个可打印字符来表示二进制数据的表示方法。由于2的6次方等于64，所以每6个比特为一个单元，对应某个可打印字符。三个字节有24个比特，对应于4个Base64单元，即3个字节需要用4个可打印字符来表示。JDK 中提供了非常方便的 BASE64Encoder 和 BASE64Decoder，用它们可以非常方便的完成基于 BASE64 的编码和解码 载荷（playload） 载荷就是存放有效信息的地方。这个名字像是特指飞机上承载的货品，这些有效信息包含三个部分 （1）标准中注册的声明（建议但不强制使用） iss: jwt签发者 sub: jwt所面向的用户 aud: 接收jwt的一方 exp: jwt的过期时间，这个过期时间必须要大于签发时间 nbf: 定义在什么时间之前，该jwt都是不可用的. iat: jwt的签发时间 jti: jwt的唯一身份标识，主要用来作为一次性token,从而回避重放攻击。 （2）公共的声明 公共的声明可以添加任何的信息，一般添加用户的相关信息或其他业务需要的必要信息. 但不建议添加敏感信息，因为该部分在客户端可解密. （3）私有的声明 私有声明是提供者和消费者所共同定义的声明，一般不建议存放敏感信息，因为base64 是对称解密的，意味着该部分信息可以归类为明文信息。 这个指的就是自定义的claim。比如前面那个结构举例中的admin和name都属于自定的 claim。这些claim跟JWT标准规定的claim区别在于：JWT规定的claim，JWT的接收方在 拿到JWT之后，都知道怎么对这些标准的claim进行验证(还不知道是否能够验证)；而 private claims不会验证，除非明确告诉接收方要对这些claim进行验证以及规则才行. 定义一个payload: {&quot;sub&quot;:&quot;1234567890&quot;,&quot;name&quot;:&quot;John Doe&quot;,&quot;admin&quot;:true} 然后将其进行base64编码，得到Jwt的第二部分。 eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9 签证（signature） jwt的第三部分是一个签证信息，这个签证信息由三部分组成 header (base64后的) payload (base64后的) secr 这个部分需要base64加密后的header和base64加密后的payload使用.连接组成的字符串，然后通过header中声明的 加密方式进行加盐secret组合加密，然后就构成了jwt的第三部分。 TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ 将这三部分用.连接成一个完整的字符串,构成了最终的jwt: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9.TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ 注意：secret是保存在服务器端的，jwt的签发生成也是在服务器端的，secret就是用来进行jwt的签发和jwt的验证， 所以，它就是你服务端的私钥，在任何场景都不应该流露出去。一旦客户端得知这个secret, 那就意味着客户端是可以自我签发jwt了。 Java的JJWT实现JWT 什么是JJWT？ JJWT是一个提供端到端的JWT创建和验证的Java库。永远免费和开源(Apache License，版本2.0)，JJWT很容易使用和理解。它被设计成一个以建筑为中心的流畅界 面，隐藏了它的大部分复杂性。 token的创建 一、引入依赖 &lt;dependency&gt; &lt;groupId&gt;io.jsonwebtoken&lt;/groupId&gt; &lt;artifactId&gt;jjwt&lt;/artifactId&gt; &lt;version&gt;0.6.0&lt;/version&gt; &lt;/dependency&gt; 二、创建类JwtTest，用于生成token /** * 测试生产Json Web Token */ public class CreateJwt { public static void main(String[] args) { /** * signWith实现签名算法和盐(huangsm是盐) * setIssuedAt用于设置签发时间 * signWith用于设置签名秘钥 * setExpiration设置过期时间 */ //为了方便测试，我们将过期时间设置为1分钟 long now = System.currentTimeMillis();// 当前时间 long exp = now + 1000 * 60;//过期时间为1分钟 JwtBuilder jwtBuilder = Jwts.builder() .setId(&quot;666&quot;) .setSubject(&quot;你号&quot;) .setIssuedAt(new Date()) .signWith(SignatureAlgorithm.HS256, &quot;huangsm&quot;) .setExpiration(new Date(exp)); System.out.println(jwtBuilder.compact()); } } token的解析 我们刚才已经创建了token ，在web应用中这个操作是由服务端进行然后发给客户 端，客户端在下次向服务端发送请求时需要携带这个token（这就好像是拿着一张门票一 样），那服务端接到这个token 应该解析出token中的信息（例如用户id）,根据这些信息 查询数据库返回相应的结果。 解析类: /** * JWT解析 * * @author huangsm */ public class ParseJwt { public static void main(String[] args) { String token = &quot;eyJhbGciOiJIUzI1NiJ9.eyJqdGkiOiI2NjYiLCJzdWIiOiLkvaDlj7ciLCJpYXQiOjE1NDg1MDI5NzZ9.-MhnLDDDmdsT6c8CX1qXA-kM2HneEgdxIrNH5_54Jn8&quot;; Claims claims = Jwts.parser().setSigningKey(&quot;huangsm&quot;) .parseClaimsJws(token).getBody(); System.out.println(&quot;用户ID:&quot;+claims.getId()); System.out.println(&quot;用户名称:&quot;+claims.getSubject()); System.out.println(&quot;登录时间:&quot;+claims.getIssuedAt()); } } 试着将token或签名秘钥篡改一下，会发现运行时就会报错，所以解析token也就是验证token 自定义claims 我们刚才的例子只是存储了id和subject两个信息，如果你想存储更多的信息（例如角色）可以定义自定义claims /** * 测试生产Json Web Token */ public class CreateJwt { public static void main(String[] args) { /** * signWith实现签名算法和盐(huangsm是盐) * setIssuedAt用于设置签发时间 * signWith用于设置签名秘钥 * setExpiration设置过期时间 * claim自定义claim */ //为了方便测试，我们将过期时间设置为1分钟 long now = System.currentTimeMillis();// 当前时间 long exp = now + 1000 * 60;//过期时间为1分钟 JwtBuilder jwtBuilder = Jwts.builder() .setId(&quot;666&quot;) .setSubject(&quot;你号&quot;) .setIssuedAt(new Date()) .signWith(SignatureAlgorithm.HS256, &quot;huangsm&quot;) .setExpiration(new Date(exp)) .claim(&quot;role&quot;,&quot;admin&quot;) .claim(&quot;image&quot;,&quot;login.png&quot;); System.out.println(jwtBuilder.compact()); } }]]></content>
      <categories>
        <category>JWT</category>
      </categories>
      <tags>
        <tag>Json Web Token</tag>
        <tag>token</tag>
        <tag>JJWT</tag>
        <tag>SpringSecurity</tag>
        <tag>认证机制</tag>
        <tag>无状态登录和有状态登录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RabbitMQ在用户微服务开发中的实战]]></title>
    <url>%2F2019%2F01%2F26%2FRabbitMQ%E5%9C%A8%E7%94%A8%E6%88%B7%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%BC%80%E5%8F%91%E4%B8%AD%E7%9A%84%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[环境准备用户微服务开发 发送短信验证码 实现思路： 在用户微服务编写API ,生成手机验证码，存入Redis并发送到RabbitMQ 一、准备工作 （1）因为要用到缓存和消息队列，所以在用户微服务（tensquare_user）引入依赖redis和amqp的起步依赖。 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; （2）修改application.yml ,在spring 节点下添加配置 spring: redis: host: 192.168.25.133 rabbitmq: host: 192.168.25.133 二、代码实现 （1）在UserService中新增方法，用于发送短信验证码 /** * 发送短信验证码 * * @param mobile 手机号 */ public void sendSms(String mobile) { //1.生成6位短信验证码 String code = RandomStringUtils.randomNumeric(6); System.out.println(mobile + &quot;收到验证码是:&quot; + code); //2.将验证码放于redis(5分钟过期) redisTemplate.opsForValue().set(&quot;smsCode_&quot; + mobile, code, 5, TimeUnit.MINUTES); //3.将验证码和手机号发动到RabbitMQ中 Map&lt;String, String&gt; map = new HashMap&lt;&gt;(2); map.put(&quot;mobile&quot;, mobile); map.put(&quot;code&quot;, code); rabbitTemplate.convertAndSend(&quot;sms&quot;, map); } （2）UserController新增方法 @PostMapping(&quot;/sendSms/{mobile}&quot;) public Result sendSms(@PathVariable(&quot;mobile&quot;) String mobile){ userService.sendSms(mobile); return new Result(true,StatusCode.OK,&quot;发送成功&quot;); } （3）启动微服务，在rabbitMQ中创建名为sms的队列，测试API 三、用户注册服务开发 /** * 用户注册 * @param user 用户信息 * @param code 验证码 */ @Transactional(rollbackFor = Exception.class) public void register(User user,String code){ String sysCode = (String) redisTemplate.opsForValue().get(&quot;smsCode_&quot; + user.getMobile()); if (StringUtils.isEmpty(sysCode)){ throw new RuntimeException(&quot;请点击获取验证码!&quot;); } if (StringUtils.isEmpty(code)){ throw new RuntimeException(&quot;请输入您收到的验证码!&quot;); } if (!sysCode.equals(code)){ throw new RuntimeException(&quot;验证码输入有误，请重新输入!&quot;); } user.setId(idWorker.nextId()+&quot;&quot;); //关注数 user.setFollowcount(0); //粉丝总数 user.setFanscount(0); userDao.save(user); } 短信微服务开发开发短信发送微服务，从rabbitMQ中提取消息，调用阿里大于短信接口实现短信发送。（我们这里实际做的就是消息的消费） 引入依赖 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt; &lt;/dependency&gt; 创建application.yml server: port: 9009 spring: application: name: tensquare-sms rabbitmq: host: 192.168.25.133 创建消息监听器(消息消费者) /** * 发送短信监听类 * @author huangsm * @date 2019/1/26 17:08:36 */ @Component @RabbitListener(queues = &quot;sms&quot;) public class SmsListener { @RabbitHandler public void sendSms(Map&lt;String,String&gt; map){ System.out.println(map); } } 这里RabbitMQ在短信服务中的应用就开发完成了，这里没有真的发送短信，如果发送短信可以调用第三方(如阿里大于等)]]></content>
      <categories>
        <category>RabbitMQ</category>
      </categories>
      <tags>
        <tag>RabbitMQ</tag>
        <tag>SpringBoot</tag>
        <tag>短信服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息中间件RabbitMQ的使用以及基本概念]]></title>
    <url>%2F2019%2F01%2F26%2F%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6RabbitMQ%E7%9A%84%E4%BD%BF%E7%94%A8%E4%BB%A5%E5%8F%8A%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[RabbitMQRabbitMQ简介 消息队列中间件简介 消息队列中间件是分布式系统中重要的组件，主要解决应用耦合，异步消息，流量削锋等问题实现高性能，高可用，可伸缩和 最终一致性[架构]使用较多的消息队列有ActiveMQ，RabbitMQ，ZeroMQ，Kafka，MetaMQ，RocketMQ 以下介绍消息队列在实际应用中常用的使用场景：异步处理，应用解耦，流量削锋和消息通讯四个场景 速度:kafka、rabbitmq、activemq最安全的是:activemq 什么是RabbitMQ RabbitMQ 是一个由 Erlang 语言开发的 AMQP 的开源实现。 AMQP ：Advanced Message Queue，高级消息队列协议。它是应用层协议的一个开放 标准，为面向消息的中间件设计，基于此协议的客户端与消息中间件可传递消息，并不 受产品、开发语言等条件的限制。 RabbitMQ 最初起源于金融系统，用于在分布式系统中存储转发消息，在易用性、扩展 性、高可用性等方面表现不俗。具体特点包括： 1.可靠性（Reliability） RabbitMQ 使用一些机制来保证可靠性，如持久化、传输确认、发布确认。 2.灵活的路由（Flexible Routing） 在消息进入队列之前，通过 Exchange 来路由消息的。对于典型的路由功能，RabbitMQ 已经提供了一些内置的 Exchange 来实现。针对更复杂的路由功能，可以将多个 Exchange 绑定在一起，也通过插件机制实现自己的 Exchange 。 3.消息集群（Clustering） 多个 RabbitMQ 服务器可以组成一个集群，形成一个逻辑 Broker 。 4.高可用（Highly Available Queues） 队列可以在集群中的机器上进行镜像，使得在部分节点出问题的情况下队列仍然可用。 5.多种协议（Multi-protocol） RabbitMQ 支持多种消息队列协议，比如 STOMP、MQTT 等等。 6.多语言客户端（Many Clients） RabbitMQ 几乎支持所有常用语言，比如 Java、.NET、Ruby 等等。 7.管理界面（Management UI） RabbitMQ 提供了一个易用的用户界面，使得用户可以监控和管理消息 Broker 的许多方 面。 8.跟踪机制（Tracing） 如果消息异常，RabbitMQ 提供了消息跟踪机制，使用者可以找出发生了什么。 9.插件机制（Plugin System） RabbitMQ 提供了许多插件，来从多方面进行扩展，也可以编写自己的插件 架构图与主要概念 RabbitMQ架构图 主要概念 RabbitMQ Server： 也叫broker server，它是一种传输服务。 他的角色就是维护一条 从Producer到Consumer的路线，保证数据能够按照指定的方式进行传输。 Producer： 消息生产者，如图A、B、C，数据的发送方。消息生产者连接RabbitMQ服 务器然后将消息投递到Exchange。 Consumer：消息消费者，如图1、2、3，数据的接收方。消息消费者订阅队列， RabbitMQ将Queue中的消息发送到消息消费者。 Exchange：生产者将消息发送到Exchange（交换器），由Exchange将消息路由到一个 或多个Queue中（或者丢弃）。Exchange并不存储消息。RabbitMQ中的Exchange有 direct、fanout、topic、headers四种类型，每种类型对应不同的路由规则。 Queue：（队列）是RabbitMQ的内部对象，用于存储消息。消息消费者就是通过订阅 队列来获取消息的，RabbitMQ中的消息都只能存储在Queue中，生产者生产消息并最终 投递到Queue中，消费者可以从Queue中获取消息并消费。多个消费者可以订阅同一个 Queue，这时Queue中的消息会被平均分摊给多个消费者进行处理，而不是每个消费者 都收到所有的消息并处理。 RoutingKey：生产者在将消息发送给Exchange的时候，一般会指定一个routing key， 来指定这个消息的路由规则，而这个routing key需要与Exchange Type及binding key联 合使用才能最终生效。在Exchange Type与binding key固定的情况下（在正常使用时一 般这些内容都是固定配置好的），我们的生产者就可以在发送消息给Exchange时，通过 指定routing key来决定消息流向哪里。RabbitMQ为routing key设定的长度限制为255 bytes Connection： （连接）：Producer和Consumer都是通过TCP连接到RabbitMQ Server 的。以后我们可以看到，程序的起始处就是建立这个TCP连接。 Channels： （信道）：它建立在上述的TCP连接中。数据流动都是在Channel中进行 的。也就是说，一般情况是程序起始建立TCP连接，第二步就是建立这个Channel。 VirtualHost：权限控制的基本单位，一个VirtualHost里面有若干Exchange和 MessageQueue，以及指定被哪些user使用 RabbitMQ的使用 RabbitMQ安装与启动 一、windows环境下的安装 （1）下载并安装 Eralng配套软件中已提供otp_win64_20.2.exe （以管理员身份运行安装） （2）下载并安装rabbitmq配套软件中已提供rabbitmq-server-3.7.4.exe。双击安装，注意不要安装在包含 中文和空格的目录下！安装后window服务中就存在rabbitMQ了，并且是启动状态。 （3）安装管理界面（插件）进入rabbitMQ安装目录的sbin目录，输入命令 rabbitmq‐plugins enable rabbitmq_management （4）重新启动服务 （5）打开浏览器，地址栏输入http://127.0.0.1:15672 ,即可看到管理界面的登陆页 二、docker容器中按照RabbitMQ （1）拉去rabbitMQ镜像 docker pull rabbitmq:management （2）创建容器，rabbitmq需要有映射以下端口: 5671 5672 4369 15671 15672 25672 15672 (if management plugin is enabled) 15671 management监听端口 5672, 5671 (AMQP 0-9-1 without and with TLS) 4369 (epmd) epmd 代表 Erlang 端口映射守护进程 25672 (Erlang distribution) (3)启动容器: docker run -d --name=myrabbitmq -p 5671:5671 -p 5672:5672 -p 4369:4369 -p 15671:15671 -p 15672:15672 -p 25672:25672 d69a5113ceae 输入http://192.168.25.133:15672进入RabbitMQ登录页面 输入用户名和密码，都为guest 进入主界 最上侧的导航以此是：概览、连接、信道、交换器、队列、用户管理 直接模式(Direct) 一、什么是Direct模式 我们需要将消息发给唯一一个节点时使用这种模式，这是最简单的一种形式。 任何发送到Direct Exchange的消息都会被转发到RouteKey中指定的Queue。 1.一般情况可以使用rabbitMQ自带的Exchange：&quot;&quot;(该Exchange的名字为空字符串，下 文称其为default Exchange)。 2.这种模式下不需要将Exchange进行任何绑定(binding)操作 3.消息传递时需要一个“RouteKey”，可以简单的理解为要发送到的队列名字。 4.如果vhost中不存在RouteKey中指定的队列名，则该消息会被抛弃。 二、创建队列 做下面的例子前，我们先建立一个叫myQue的队列 Durability：是否做持久化 Durable（持久） transient（临时） Auto delete : 是否自动删除 三、代码实现-消息生产者 1.引入依赖 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt; &lt;/dependency&gt; 2.编写配置文件application.yml spring: rabbitmq: host: 192.168.25.133 3.编写测试类 @Autowired private RabbitTemplate rabbitTemplate; @Test public void contextLoads() { /** * 第一个参数是队列名也就是routingKey */ rabbitTemplate.convertAndSend(&quot;myQue&quot;,&quot;测试下直接模式(Direct)&quot;); } 四、代码实现-消息消费者 （1）编写消息消费者类 @Component @RabbitListener(queues = &quot;myQue&quot;) public class Customer { @RabbitHandler public void showMessage(String msg){ System.out.println(&quot;myQue接收的消息:&quot;+msg); } } (2)运行启动类，可以在控制台看到刚才发送的消息 利用IDEA启用俩个消费者实例，在利用生产者发送消息，多次测试后发现俩个消费者实例收到消息的比例相同为1:1（负载均衡） 分列模式（Fanout） 一、什么是分列（Fanout）模式 当我们需要将消息一次发给多个队列时，需要使用这种模式。 任何发送到Fanout Exchange的消息都会被转发到与该Exchange绑定(Binding)的所有 Queue上。 1.可以理解为路由表的模式 2.这种模式不需要RouteKey 3.这种模式需要提前将Exchange与Queue进行绑定，一个Exchange可以绑定多个 Queue，一个Queue可以同多个Exchange进行绑定。 4.如果接受到消息的Exchange没有与任何Queue绑定，则消息会被抛弃。 创建交换机 绑定QUEUE 二、代码实现---分列模式消息生产者 /** * * 分列模式(Fanout) */ @Test public void sendMsg2() { /** * 第一个参数是交换机名称，第二个队列名称是routingkey */ rabbitTemplate.convertAndSend(&quot;huangsm&quot;,&quot;&quot;,&quot;测试分列模式(Fanout)&quot;); } 三、代码实现-分列模式消息消费者 @Component @RabbitListener(queues = &quot;myQue2&quot;) public class Customer2 { @RabbitHandler public void showMessage(String msg){ System.out.println(&quot;myQue3接收的消息:&quot;+msg); } } 主题模式（Topic） 一、什么是主题模式 任何发送到Topic Exchange的消息都会被转发到所有关心RouteKey中指定话题的Queue上 由上图看出: 此类交换器使得来自不同的源头的消息可以到达一个队列，其实说的更明白一点就是模 糊匹配的意思，例如：上图中红色对列的routekey为usa.#，#代表匹配任意字符，但是 要想消息能到达此对列，usa.必须匹配后面的#好可以随意。图中usa.news usa.weather,都能找到红色队列，符号 # 匹配一个或多个词，符号 * 匹配不多不少一个 词。因此 usa.# 能够匹配到 usa.news.XXX ，但是 usa.* 只会匹配到 usa.XXX 。 注： 交换器说到底是一个名称与队列绑定的列表。当消息发布到交换器时，实际上是由你所 连接的信道，将消息路由键同交换器上绑定的列表进行比较，最后路由消息。 任何发送到Topic Exchange的消息都会被转发到所有关心RouteKey中指定话题的 Queue上 1.这种模式较为复杂，简单来说，就是每个队列都有其关心的主题，所有的消息都带有一 个“标题”(RouteKey)，Exchange会将消息转发到所有关注主题能与RouteKey模糊匹配的 队列。 2.这种模式需要RouteKey，也许要提前绑定Exchange与Queue。 3.在进行绑定时，要提供一个该队列关心的主题，如“#.log.#”表示该队列关心所有涉及 log的消息(一个RouteKey为”MQ.log.error”的消息会被转发到该队列)。 4.“#”表示0个或若干个关键字，“”表示一个关键字。如“log.”能与“log.warn”匹配，无法 与“log.warn.timeout”匹配；但是“log.#”能与上述两者匹配。 5.同样，如果Exchange没有发现能够与RouteKey匹配的Queue，则会抛弃此消息 创建队列与绑定 （1）新建一个交换器 ，类型选择topic （2）点击新建的交换器topichuang,添加匹配规则，添加后列表如下： 消息生产者代码已经消费者测试结果 /** * 测试主题模式(topic) */ @Test public void sendMsg3() { /** * 第一个参数是交换机名称，第二个队列名称是routingKey */ rabbitTemplate.convertAndSend(&quot;topichuang&quot;,&quot;huang.aaa&quot;,&quot;aaa消息发送给谁呢&quot;); } /** * 测试主题模式(topic) */ @Test public void sendMsg4() { /** * 第一个参数是交换机名称，第二个队列名称是routingKey */ rabbitTemplate.convertAndSend(&quot;topichuang&quot;,&quot;huang.log&quot;,&quot;huang.log消息发送给谁呢&quot;); } /** * 测试主题模式(topic) */ @Test public void sendMsg5() { /** * 第一个参数是交换机名称，第二个队列名称是routingKey */ rabbitTemplate.convertAndSend(&quot;topichuang&quot;,&quot;log.ccc&quot;,&quot;ccc消息发送给谁呢&quot;); } Topic模式测试结果 Topic模式流程图 结束语这篇博客主要讲述RabbitMQ的一些概念和一些操作方式，下篇博客会介绍RabbitMQ的实战，开发一个用户微服务]]></content>
      <categories>
        <category>RabbitMQ</category>
      </categories>
      <tags>
        <tag>RabbitMQ</tag>
        <tag>主题模式Topic</tag>
        <tag>直接模式Direct</tag>
        <tag>分列模式（Fanout)</tag>
        <tag>AMQP</tag>
        <tag>Erlang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker下部署elasticsearch环境]]></title>
    <url>%2F2019%2F01%2F25%2Fdocker%E4%B8%8B%E9%83%A8%E7%BD%B2elasticsearch%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[环境准备 阿里云服务器 docker最新版环境 部署步骤容器的创建与远程连接 下载镜像 docker pull elasticsearch:5.6.8 创建容器 docker run -d --name es -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; 6c0bdf761f3b 如果启动报错，通过docker logs es查看日志，如果错误是 Java HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000085330000, 2060255232, 0) failed; error=’Cannot allocate memory’ (errno=12) 因为这是由于elasticsearch5.0默认分配jvm空间大小为2g，内存不足以分配导致修改elasticsearch分配的jvm空间 find /var/lib/docker/overlay2/ -name jvm.options vim /var/lib/docker/overlay2/7f59cf980d035aa1f8e5275e4e64eb9ec9a775b0d62bacf94ee3d562b782c136/diff/etc/elasticsearch/jvm.options 最后重新启动即可 测试 输入http://192.168.25.133:9200/ 使用写好的搜索服务，改变elasticsearch服务器地址后启动测试报错: 进入容器 一、输入命令 docker exec es -it /bin/bash 此时，我们看到elasticsearch所在的目录为/usr/share/elasticsearch, 进入config看到了配置文件elasticsearch.yml 我们通过vi命令编辑此文件，尴尬的是容器并没有vi命令 ，咋办？我们需要以文件挂载的 方式创建容器才行，这样我们就可以通过修改宿主机中的某个文件来实现对容器内配置文件的修改 二、拷贝配置文件到宿主机 首先退出容器，然后执行命令： docker cp es:/usr/share/elasticsearch/config/elasticsearch.yml /usr/share/elasticsearch.yml 三、停止和删除原来创建的容器 docker stop es docker rm es 四、重新执行创建容器命令 docker run -d --name=es -p 9200:9200 -p 9300:9300 -v /usr/share/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml elasticsearch:5.6.8 五、修改/usr/share/elasticsearch.yml 将 transport.host: 0.0.0.0 前的#去掉后保存文件退出。 其作用是允许任何ip地址访问elasticsearch开发测试阶段可以这么做，生产环境下指定具体的IP 重启启动 docker restart es重新启动失败怎么办? 重启后发现重启启动失败了，这时什么原因呢？这与我们刚才修改的配置有关，因为elasticsearch在启动的时候会进行一些检查， 比如最多打开的文件的个数以及虚拟内存区域数量等等，如果你放开了此配置，意味着需要打开更多的文件以及虚拟内存， 所以我们还需要系统调优。 系统调优(宿主机问题) 我们一共需要修改两处 第一步:修改/etc/security/limits.conf 配置文件,追加 * soft nofile 65536 * hard nofile 655 nofile是单个进程允许打开的最大文件个数 soft nofile 是软限制 hard nofile是硬限制 第二步:修改/etc/sysctl.conf，追加内容 vm.max_map_count=655360 限制一个进程可以拥有的VMA(虚拟内存区域)的数量 执行下面命令 修改内核参数马上生效 安装IK分词器 （1）快捷键alt+p进入sftp , 将ik文件夹上传至宿主机 put -r D:\friendprojec\elasticsearch-5.6.8\plugins\ik （2）在宿主机中将ik文件夹拷贝到容器内 /usr/share/elasticsearch/plugins 目录下。 docker cp ik es:/usr/share/elasticsearch/plugins/ （3）重启容器 HEAD插件安装 （1）修改/usr/share/elasticsearch.yml ,添加允许跨域配置 http.cors.enabled: true http.cors.allow‐origin: &quot;*&quot; （2）重新启动elasticseach容器（3）下载head镜像 docker pull mobz/elasticsearch‐head:5 （4）创建head容器 docker run ‐di ‐‐name=myhead ‐p 9100:9100 mobz/elasticsearch‐head:5 这样我们在docker中部署elasticsearch就完成了]]></content>
      <categories>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>elasticsearch</tag>
        <tag>head</tag>
        <tag>IK</tag>
        <tag>docker系统优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Logstash同步ElasticSearch与Mysql数据]]></title>
    <url>%2F2019%2F01%2F24%2F%E4%BD%BF%E7%94%A8Logstash%E5%90%8C%E6%AD%A5ElasticSearch%E4%B8%8EMysql%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[ElasticSearch与MYSQL数据库的同步(使用logstash) 概述 Logstash是一款轻量级的日志搜集处理框架，可以方便的把分散的、多样化的日志搜集起来，并进行自定义的理，然后传输 到指定的位置，比如某个服务器或者文件。 Logstash的安装和测试 解压，进入bin目录 logstash ‐e &apos;input { stdin { } } output { stdout {} }&apos; 控制台输入字符，随后就有日志输出 stdin，表示输入流，指从键盘输入 stdout，表示输出流，指从显示器输出 命令行参数: -e 执行 --config 或 -f 配置文件，后跟参数类型可以是一个字符串的配置或全路径文件名或全路径 (如：/etc/logstash.d/，logstash会自动读取/etc/logstash.d/目录下所有*.conf 的文本文件， 然后在自己内存里拼接成一个完整的大配置文件再去执行) MySQL数据导入Elasticsearch 第一步: 在logstansh根目录下创建mysqletc(名称任意)目录，目录中创建mysql.conf（名字任意）文件，已经mysql的驱动jar包 第二步: 编写mysql.conf文件 input { jdbc { # mysql jdbc connection string to our backup databse jdbc_connection_string =&gt; &quot;jdbc:mysql://47.107.44.169:33061/tensquare_article?characterEncoding=UTF8&quot; # the user we wish to excute our statement as jdbc_user =&gt; &quot;root&quot; jdbc_password =&gt; &quot;123456&quot; # the path to our downloaded jdbc driver jdbc_driver_library =&gt; &quot;D:\friendprojec\logstash-5.6.8\mysqletc\mysql-connector-java-5.1.46.jar&quot; # the name of the driver class for mysql jdbc_driver_class =&gt; &quot;com.mysql.jdbc.Driver&quot; jdbc_paging_enabled =&gt; &quot;true&quot; jdbc_page_size =&gt; &quot;50&quot; #以下对应着要执行的sql的绝对路径。 #statement_filepath =&gt; &quot;&quot; statement =&gt; &quot;SELECT id,title,content,state FROM tb_article&quot; #定时字段 各字段含义（由左至右）分、时、天、月、年，全部为*默认含义为每分钟都更新（测试结果，不同的话请留言指出） schedule =&gt; &quot;* * * * *&quot; } } output { elasticsearch { #ESIP地址与端口 hosts =&gt; &quot;127.0.0.1:9200&quot; #ES索引名称（自己定义的） index =&gt; &quot;tensquare_article&quot; #自增ID编号 document_id =&gt; &quot;%{id}&quot; document_type =&gt; &quot;article&quot; } stdout { #以JSON格式输出 codec =&gt; json_lines } } 第三步: 启动logstansh，用-f的方式 logstash -f ../mysqletc/mysql.conf 注意mysql.conf路径为相对路径 logstansh一些注意的问题 logstansh支持多数据库同步ElasticSearch 这就是logstansh同步数据库的基础使用，也可以部署logstansh集群配合kafka或redis做到缓存同步效果]]></content>
      <categories>
        <category>Logstash</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
        <tag>Logstash</tag>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch基础入门及搜索服务的开发]]></title>
    <url>%2F2019%2F01%2F23%2FElasticSearch%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E5%8F%8A%E6%90%9C%E7%B4%A2%E6%9C%8D%E5%8A%A1%E7%9A%84%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[概述什么是ElasticSearchElasticsearch是一个实时的分布式搜索和分析引擎。它可以帮助你用前所未有的速度去处理大规模数据。 ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引 擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。 ElasticSearch特点（1）可以作为一个大型分布式集群（数百台服务器）技术，处理PB级数据，服务大公司；也可以运行在单机上 （2）将全文检索、数据分析以及分布式技术，合并在了一起，才形成了独一无二的ES； （3）开箱即用的，部署简单 （4）全文检索，同义词处理，相关度排名，复杂数据分析，海量数据的近实时 ElasticSearch体系结构 环境准备ElasticSearch部署与启动 下载ElasticSearch 5.6.8版本 https://www.elastic.co/downloads/past-releases/elasticsearch-5-6-8 在命令提示符下，进入ElasticSearch安装目录下的bin目录,执行命令即可启动。 我们打开浏览器，在地址栏输入http://127.0.0.1:9200/ 即可看到输出结果 不同操作方式使用ElasticSearch通过PostMan使用RestFul风格操作ElasticSearch 新建索引库 新建文档 查询全部文档 修改文档如果ID不存在则重新创建 按ID查询文档 基本匹配查询 模糊查询 删除文档Head插件方式操控ElasticSearch安装head 下载head插件 https://github.com/mobz/elasticsearch-head 解压到任意目录，但是要和elasticsearch的安装目录区别开 按照node js，按照cnpm npm install -g cnpm --registry=https://registry.npm.taobao.org 将grunt安装为全局命令 。Grunt是基于Node.js的项目构建工具。它可以自动运行你所设定的任务 npm install ‐g grunt‐cli 安装依赖 cnpm install 进入head目录启动head，在命令提示符下输入命令 grunt server 打开浏览器，输入 http://localhost:9100 点击连接按钮没有任何相应，按F12发现有如下错误 No &apos;Access-Control-Allow-Origin&apos; header is present on the requested resour 这个错误是由于elasticsearch默认不允许跨域调用，而elasticsearch-head是属于前端工程，所以报错。 我们这时需要修改elasticsearch的配置，让其允许跨域访问。 修改elasticsearch配置文件：elasticsearch.yml，增加以下两句命令： http.cors.enabled: true http.cors.allow‐origin: &quot;*&quot; 此步为允许elasticsearch跨越访问 点击连接即可看到相关信息 IK分词器的使用 安装IK分词器下载地址：https://github.com/medcl/elasticsearch-analysis-ik/releases 下载5.6.8版本 （1）先将其解压，将解压后的elasticsearch文件夹重命名文件夹为ik （2）将ik文件夹拷贝到elasticsearch/plugins 目录下。 （3）重新启动，即可加载IK分词器 IK分词器测试IK提供了两个分词算法ik_smart 和 ik_max_word其中ik_smart为最少切分，ik_max_word为最细粒度划分我们分别来试一下 （1）最小切分：在浏览器地址栏输入地址 http://127.0.0.1:9200/_analyze?analyzer=ik_smart&amp;pretty=true&amp;text=我是中国程序员 输出的结果为： （2）最细切分：在浏览器地址栏输入地址 http://127.0.0.1:9200/_analyze?analyzer=ik_max_word&amp;pretty=true&amp;text=我是中国程序员 输出结果为: IK自定义词库步骤： （1）进入elasticsearch/plugins/ik/config目录 （2）新建一个my.dic文件，编辑内容 （3）修改IKAnalyzer.cfg.xml（在ik/config目录下）,然后重启elasticsearch服务 注意my.dic文件需要存储为UTF-8无BOM格式 JAVA操作ElasticSearch 关于索引库文档类概念 是否索引，就是看该域是否能被搜索 是否分词，就表示搜索的时候是整体匹配还是单词匹配，如果不分词的话代表整句匹配 是否存储，就是是否在页面上显示 引入依赖和修改配置文件 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-elasticsearch&lt;/artifactId&gt; &lt;/dependency&gt; 修改配置文件: spring: data: elasticsearch: cluster-nodes: localhost:9300 创建索引文档类 /** * SpringDataElasticSearch索引库文档类 * indexName 索引库名称 * type 数据库类型 * @author huangsm */ @Document(indexName = &quot;tensquare_article&quot;,type = &quot;article&quot;) @Data public class Article implements Serializable { @Id private String id; /** * index 是否索引，就是看该域是否能被搜索 * 是否分词，就表示搜索的时候是整体匹配还是单词匹配，如果不分词的话代表整句匹配 * 是否存储，就是是否在页面上显示 */ @Field(index = true, analyzer = &quot;ik_smart&quot;,searchAnalyzer = &quot;ik_smart&quot;) private String title; @Field(index = true, analyzer = &quot;ik_smart&quot;,searchAnalyzer = &quot;ik_smart&quot;) private String content; /** * 审核状态 */ private String state; } 编写文章持久层 public interface ArticleSearchDao extends ElasticsearchRepository&lt;Article,String&gt; { /** * 检索 * @param title * @param content * @param pageable * @return */ public Page&lt;Article&gt;findByTitleOrContentLike(String title, String content, Pageable pageable); } 编写业务层 /** * 文章搜索业务 * @author huangsm */ @Service public class ArticleSearchService { @Autowired private ArticleSearchDao articleSearchDao; @Autowired private IdWorker idWorker; /** * 增加文章 * @param article */ public void add(Article article){ article.setId(idWorker.nextId()+&quot;&quot;); articleSearchDao.save(article); } public Page&lt;Article&gt;findByTitleLike(String keywords,int page,int size){ PageRequest pageRequest = PageRequest.of(page - 1, size); return articleSearchDao.findByTitleOrContentLike(keywords,keywords,pageRequest); } } 编写控制层 /** * 搜索controller * * @author huangsm */ @RestController @CrossOrigin @RequestMapping(&quot;/article&quot;) public class ArticleController { @Autowired private ArticleSearchService articleSearchService; @PostMapping(&quot;/&quot;) public Result save(@RequestBody Article article) { articleSearchService.add(article); return new Result(true, StatusCode.OK, &quot;操作成功&quot;); } @GetMapping(value = &quot;/search/{keywords}/{page}/{size}&quot;) public Result findByTitleLike(@PathVariable(&quot;keywords&quot;) String keywords, @PathVariable(&quot;page&quot;) int page, @PathVariable(&quot;size&quot;) int size) { Page&lt;Article&gt; pageData = articleSearchService.findByTitleLike(keywords, page, size); return new Result(true, StatusCode.OK, &quot;查询成功&quot;, new PageResult&lt;Article&gt;(pageData.getTotalElements(), pageData.getContent())); } } 测试 ElasticSearch和Solr的对比ES和Solr的对比]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>SpringData</tag>
        <tag>ElasticSearch</tag>
        <tag>head</tag>
        <tag>IK分词器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringDataMongoDB的基本使用及服务开发]]></title>
    <url>%2F2019%2F01%2F23%2FSpringDataMongoDB%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E5%8F%8A%E6%9C%8D%E5%8A%A1%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[SpringDataMongoDb概述 SpringData家族成员之一，用于操作MongoDb的持久层框架，封装了底层的mongodb-driver 快速入门 引入依赖 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-mongodb&lt;/artifactId&gt; &lt;/dependency&gt; 配置application.yml配置文件 spring: data: mongodb: host: ip地址 database: 数据库名称 通过IOC注入SpringDataMongoDB封装的模版 在业务层注入： @Autowired private MongoTemplate mongoTemplate; //根据id，修改huangsm表下的num字段，每次加1，代码如下 Query query = new Query(); query.addCriteria(Criteria.where(&quot;_id&quot;).is(id)); Update update = new Update(); update.inc(&quot;num&quot;, 1); mongoTemplate.updateFirst(query, update, &quot;huangsm&quot;); 关于服务开发首先引入依赖，配置配置文件，然后引入MongoDB模版，剩余操作参考根据业务来选择SpringDataMongoDB提供的API来完成业务。本文只是一个简单的入门，更为详细建议参考官方文档，其实笔者认为MongoDB主要使用与数据价值低而且数据量大的场景，掌握常用的CURD就可以玩转Java中MongoDB的使用了，SpringDataMongoDb是个不错的操作MongoDb的框架，推荐。]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
        <tag>NOSQL</tag>
        <tag>SpringData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简述HTTP和RPC的优缺点]]></title>
    <url>%2F2019%2F01%2F20%2F%E7%AE%80%E8%BF%B0HTTP%E5%92%8CRPC%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9%2F</url>
    <content type="text"><![CDATA[在HTTP和RPC的选择上，可能有些人是迷惑的，主要是因为，有些RPC框架配置复杂，如果走HTTP也能完成同样的功能，那么为什么要选择RPC，而不是更容易上手的HTTP来实现了。 本文主要来阐述HTTP和RPC的异同，让大家更容易根据自己的实际情况选择更适合的方案。 传输协议 RPC，可以基于TCP协议，也可以基于HTTP协议 HTTP，基于HTTP协议 传输效率 RPC，使用自定义的TCP协议，可以让请求报文体积更小，或者使用HTTP2协议，也可以很好的减少报文的体积，提高传输效率 HTTP，如果是基于HTTP1.1的协议，请求中会包含很多无用的内容，如果是基于HTTP2.0，那么简单的封装以下是可以作为一个RPC来使用的，这时标准RPC框架更多的是服务治理 性能消耗，主要在于序列化和反序列化的耗时 RPC，可以基于thrift实现高效的二进制传输 HTTP，大部分是通过json来实现的，字节大小和序列化耗时都比thrift要更消耗性能 负载均衡 RPC，基本都自带了负载均衡策略 HTTP，需要配置Nginx，HAProxy来实现 服务治理（下游服务新增，重启，下线时如何不影响上游调用者） RPC，能做到自动通知，不影响上游 HTTP，需要事先通知，修改Nginx/HAProxy配置 总结： RPC主要用于公司内部的服务调用，性能消耗低，传输效率高，服务治理方便。HTTP主要用于对外的异构环境，浏览器接口调用，APP接口调用，第三方接口调用等。 博客地址: https://www.babywang.huangsm.xyz Git地址: https://github.com/babybabywang 码云地址: https://gitee.com/wangyuanbaby]]></content>
      <categories>
        <category>服务发现</category>
      </categories>
      <tags>
        <tag>SpringCloud</tag>
        <tag>Dubbo</tag>
        <tag>HTTP</tag>
        <tag>RPC</tag>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB的基本使用姿势及概念]]></title>
    <url>%2F2019%2F01%2F20%2FMongoDB%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E5%A7%BF%E5%8A%BF%E5%8F%8A%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[文档型数据库MongoDBMongoDB的特点和体系结构MongoDB简介1. 什么是MongoDB MongoDB 是一个跨平台的，面向文档的数据库，是当前 NoSQL 数据库产品中最热门的一种。它介于关系数据库和非关系数据库之间， 是非关系数据库当中功能最丰富，最像关系数据库的产品。它支持的数据结构非常松散，是类似 JSON的 BSON 格式，因此可以存储比较复杂 的数据类型。 MongoDB 的官方网站地址是：http://www.mongodb.or 2. MongoDB特点 MongoDB 最大的特点是他支持的查询语言非常强大，其语法有点类似于面向对象的查询语言，几乎可以实现类似关系 数据库单表查询的绝大部分功能，而且还支持对数据建立索引。它是一个面向集合的,模式自由的文档型数据库。 具体特点总结如下： （1）面向集合存储，易于存储对象类型的数据 （2）模式自由 （3）支持动态查询 （4）支持完全索引，包含内部对象 （5）支持复制和故障恢复 （6）使用高效的二进制数据存储，包括大型对象（如视频等） （7）自动处理碎片，以支持云计算层次的扩展性 （8）支持 Python，PHP，Ruby，Java，C，C#，Javascript，Perl 及 C++语言的驱动程 序，社区中也提供了对 Erlang 及.NET 等平台的驱动程序 （9） 文件存储格式为 BSON（一种 JSON 的扩展） 3. 什么时候使用MongoDB （1）数据量大 （2）写入操作频繁 （3）价值较低 对于这样的数据，我们更适合使用MongoDB来实现数据的 4. MongoDB体系结构 MongoDB 的逻辑结构是一种层次结构。主要由： 文档(document)、集合(collection)、数据库(database)这三部分组成的。逻辑结构是面 向用户的，用户使用 MongoDB 开发应用程序使用的就是逻辑结构。 （1）MongoDB 的文档（document），相当于关系数据库中的一行记录。 （2）多个文档组成一个集合（collection），相当于关系数据库的表。 （3）多个集合（collection），逻辑上组织在一起，就是数据库（database）。 （4）一个 MongoDB 实例支持多个数据库（database）。 5. 数据类型 基本数据类型 null：用于表示空值或者不存在的字段，{“x”:null} 布尔型：布尔类型有两个值true和false，{“x”:true} 数值：shell默认使用64为浮点型数值。{“x”：3.14}或{“x”：3}。对于整型值，可以使用NumberInt（4字节符号整数）或NumberLong（8字节符号整数），{“x”:NumberInt(“3”)}{“x”:NumberLong(“3”)} 字符串：UTF-8字符串都可以表示为字符串类型的数据，{“x”：“呵呵”} 日期：日期被存储为自新纪元依赖经过的毫秒数，不存储时区，{“x”:new Date()}北京市昌平区建材城西路金燕龙办公楼一层 电话：400-618-9090 正则表达式：查询时，使用正则表达式作为限定条件，语法与JavaScript的正则表达式相同，{“x”:/[abc]/} 数组：数据列表或数据集可以表示为数组，{“x”： [“a“，“b”,”c”]} 内嵌文档：文档可以嵌套其他文档，被嵌套的文档作为值来处理，{“x”:{“y”:3 }} 对象Id：对象id是一个12字节的字符串，是文档的唯一标识，{“x”: objectId() } 二进制数据：二进制数据是一个任意字节的字符串。它不能直接在shell中使用。如果要将非utf-字符保存到数据库中，二进制数据是唯一的方式。 代码：查询和文档中可以包括任何JavaScript代码，{“x”:function(){/…/ Mysql和MongoDB的区别: 常用的MongoDB命令 首先说下Windows端mongodb的启动 md e:/data mongod –dbpath=e:/data 在b端cmd中输入 mongo即可启动 docker中安装mongoDB 拉取镜像docker pull mongo 启动容器docker run -d –name mymongodb -p 27017:27017 mongo 常用的MongoDB命令 使用JAVA操作MongoDBmongodb-driver是mongo官方推出的java连接mongoDB的驱动包，相当于JDBC驱动。我们通过一个入门的案例来了解mongodb-driver的基本 引入依赖 &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;mongodb‐driver&lt;artifactId&gt; &lt;version&gt;3.6.3&lt;/version&gt; &lt;/dependency&gt; 测试 1. 查询全部数据 /** * 测试Java操作mongoDb * @author huangsm */ public class MongoDbDemo { /** * 查询mongodb中全部数据 * @param args */ public static void main(String[] args) { //创建连接 MongoClient mongoClient = new MongoClient(&quot;47.107.44.169&quot;); //打开数据库 MongoDatabase spitdb = mongoClient.getDatabase(&quot;spitdb&quot;); //获取集合 MongoCollection&lt;Document&gt; spit = spitdb.getCollection(&quot;spit&quot;); //查询记录获取文档集合 FindIterable&lt;Document&gt; documents = spit.find(); for (Document document : documents) { System.out.println(&quot;内容:&quot;+document.getString(&quot;content&quot;)); System.out.println(&quot;访客:&quot;+document.getInteger(&quot;visits&quot;)); } mongoClient.close(); } } 2. 条件查询 /** * 测试条件查询 * @author huangsm */ public class MongoDbDemo1 { /** * 根据条件查询 * BasicDBObject对象：表示一个具体的记录，BasicDBObject实现了DBObject，是keyvalue的数据结构，用起来和HashMap是基本一致的。 * @param args */ public static void main(String[] args) { //创建连接 MongoClient mongoClient = new MongoClient(&quot;47.107.44.169&quot;); //打开数据库 MongoDatabase spitdb = mongoClient.getDatabase(&quot;spitdb&quot;); //获取集合 MongoCollection&lt;Document&gt; spit = spitdb.getCollection(&quot;spit&quot;); BasicDBObject basicDBObject=new BasicDBObject(&quot;_id&quot;,&quot;ObjectId(\&quot;5c446ba292035401c76895f0\&quot;)&quot;); //查询记录获取文档集合 FindIterable&lt;Document&gt; documents = spit.find(); for (Document document : documents) { System.out.println(&quot;id:&quot;+document.getObjectId(&quot;_id&quot;)); System.out.println(&quot;内容:&quot;+document.getString(&quot;content&quot;)); System.out.println(&quot;访客:&quot;+document.getInteger(&quot;visits&quot;)); } mongoClient.close(); } } 3. 插入数据 /** * 插入数据 * @author huangsm */ public class MongoDbDemo2 { /** * 插入数据 * @param args */ public static void main(String[] args) { //创建连接 MongoClient mongoClient = new MongoClient(&quot;47.107.44.169&quot;); //打开数据库 MongoDatabase spitdb = mongoClient.getDatabase(&quot;spitdb&quot;); //获取集合 MongoCollection&lt;Document&gt; spit = spitdb.getCollection(&quot;spit&quot;); HashMap&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;(); map.put(&quot;content&quot;,&quot;git&quot;); map.put(&quot;userId&quot;,&quot;222&quot;); map.put(&quot;visits&quot;,123); Document document = new Document(map); spit.insertOne(document); mongoClient.close(); } } 总结 总的来说MongoDB是一款非关系性的文档数据库，它适合存储那种存储价值低而且数据量大的数据。总的来说操作起来也是否的便捷，下篇文章我会使用SpringDataMongoDB来开发一个微服务。]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
        <tag>JAVA操作</tag>
        <tag>NOSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于吞吐量(TPS)、QPS、并发数、响应时间(RT)的概念]]></title>
    <url>%2F2019%2F01%2F19%2F%E5%85%B3%E4%BA%8E%E5%90%9E%E5%90%90%E9%87%8F-TPS-%E3%80%81QPS%E3%80%81%E5%B9%B6%E5%8F%91%E6%95%B0%E3%80%81%E5%93%8D%E5%BA%94%E6%97%B6%E9%97%B4-RT-%E7%9A%84%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[概述因为开发的原因，需要对吞吐量（TPS）、QPS、并发数、响应时间（RT）几个概念做下了解，查自百度百科，记录如下： 1.响应时间(RT) 响应时间是指系统对请求作出响应的时间。直观上看，这个指标与人对软件性能的主观感受是非常一致的，因为它完整地记录了整个计算机系统处理请求的时间。由于一个系统通常会提供许多功能，而不同功能的处理逻辑也千差万别，因而不同功能的响应时间也不尽相同，甚至同一功能在不同输入数据的情况下响应时间&gt;也不相同。所以，在讨论一个系统的响应时间时，人们通常是指该系统所有功能的平均时间或者所有功能的最大响应时间。当然，往往也需要对每个或每组功能讨论&gt;其平均响应时间和最大响应时间。 对于单机的没有并发操作的应用系统而言，人们普遍认为响应时间是一个合理且准确的性能指标。需要指出的是，响应时间的绝对值并不能直接反映软 件的性能的高低，软件性能的高低实际上取决于用户对该响应时间的接受程度。对于一个游戏软件来说，响应时间小于100毫秒应该是不错的，响应时 间在1秒左右可能属于勉强可以接受，如果响应时间达到3秒就完全难以接受了。而对于编译系统来说，完整编译一个较大规模软件的源代码可能需要几 十分钟甚至更长时间，但这些响应时间对于用户来说都是可以接受的。 2.吞吐量(TPS) 吞吐量是指系统在单位时间内处理请求的数量。对于无并发的应用系统而言，吞吐量与响应时间成严格的反比关系，实际上此时吞吐量就是响应时间的倒数。前面已经说过，对于单用户的系统，响应时间（或者系统响应时间和应用延迟时间）可以很好地度量系统的性能，但对于并发系统，通常需要用吞吐量作为性能指标。 &nbsp;&nbsp;&nbsp;&nbsp;对于一个多用户的系统，如果只有一个用户使用时系统的平均响应时间是t，当有你n个用户使用时，每个用户看到的响应时间通常并不是n×t，而往往比n×t小很多（当然，在某些特殊情况下也可能比n×t大，甚至大很多）。这是因为处理每个请求需要用到很多资源，由于每个请求的处理过程中有许多不走难以并发执行，这导致在具体的一个时间点，所占资源往往并不多。也就是说在处理单个请求时，在每个时间点都可能有许多资源被闲置，当处理多个请求时，如果资源配置合理，每个用户看到的平均响应时间并不随用户数的增加而线性增加。实际上，不同系统的平均响应时间随用户数增加而增长的速度也不大相同，这也是采用吞吐量来度量并发系统的性能的主要原因。一般而言，吞吐量是一个比较通用的指标，两个具有不同用户数和用户使用模式的系统，如果其最大吞吐量基本一致，则可以判断两个系统的处理能力基本一致。 3.并发数 并发用户数是指系统可以同时承载的正常使用系统功能的用户的数量。与吞吐量相比，并发用户数是一个更直观但也更笼统的性能指标。实际上，并发用户数是一个非常不准确的指标，因为用户不同的使用模式会导致不同用户在单位时间发出不同数量的请求。一网站系统为例，假设用户只有注册后才能使用，但注册用户并不是每时每刻都在使用该网站，因此具体一个时刻只有部分注册用户同时在线，在线用户就在浏览网站时会花很多时间阅读网站上的信息，因而具体一个时刻只有部分在线用户同时向系统发出请求。这样，对于网站系统我们会有三个关于用户数的统计数字：注册用户数、在线用户数和同时发请求用户数。由于注册用户可能长时间不登陆网站，使用注册用户数作为性能指标会造成很大的误差。而在线用户数和同事发请求用户数都可以作为性能指标。相比而言，以在线用户作为性能指标更直观些，而以同时发请求用户数作为性能指标更准确些。 4.QPS每秒查询率(Query Per Second) &gt;每秒查询率QPS是对一个特定的查询服务器在规定时间内所处理流量多少的衡量标准，在因特网上，作为域名系统服务器的机器的性能经常用每秒查询率来衡量。对应fetches/sec，即每秒的响应请求数，也即是最大吞吐能力。 （看来是类似于TPS，只是应用于特定场景的吞吐量）]]></content>
      <categories>
        <category>系统概念</category>
      </categories>
      <tags>
        <tag>系统概述</tag>
        <tag>概念分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RateLimiter配合注解的使用]]></title>
    <url>%2F2019%2F01%2F19%2FRateLimiter%E9%85%8D%E5%90%88%E6%B3%A8%E8%A7%A3%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[首先我们来了解下什么是RateLimiter RateLimiter 从概念上来讲，速率限制器会在可配置的速率下分配许可证。如果必要的话，每个acquire() 会阻塞当前线程直到许可证可用后获取该许可证。一旦获取到许可证，不需要再释放许可证。校对注：RateLimiter使用的是一种叫令牌桶的流控算法，RateLimiter会按照一定的频率往桶里扔令牌，线程拿到令牌才能执行，比如你希望自己的应用程序QPS不要超过1000，那么RateLimiter设置1000的速率后，就会每秒往桶里扔1000个令牌。 RateLimiter经常用于限制对一些物理资源或者逻辑资源的访问速率。与Semaphore 相比，Semaphore 限制了并发访问的数量而不是使用速率。（注意尽管并发性和速率是紧密相关的，比如参考Little定律） 通过设置许可证的速率来定义RateLimiter。在默认配置下，许可证会在固定的速率下被分配，速率单位是每秒多少个许可证。为了确保维护配置的速率，许可会被平稳地分配，许可之间的延迟会做调整。可能存在配置一个拥有预热期的RateLimiter 的情况，在这段时间内，每秒分配的许可数会稳定地增长直到达到稳定的速率。举例来说明如何使用RateLimiter，想象下我们需要处理一个任务列表，但我们不希望每秒的任务提交超过两个： //速率是每秒两个许可 final RateLimiter rateLimiter = RateLimiter.create(2.0); void submitTasks(List tasks, Executor executor) { for (Runnable task : tasks) { rateLimiter.acquire(); // 也许需要等待 executor.execute(task); } } 进入正题准备工作 引入依赖 &lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;25.1-jre&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.28&lt;/version&gt; &lt;/dependency&gt; 创建自定义注解 import java.lang.annotation.*; /** * 自定义RateLimiter限流注解 * @author huang * @PACKAGE_NAME cn.huangsm.advance.ratelimiter * @PROJECT_NAME advance-code * @date 2019/1/19 */ @Inherited @Documented @Target({ElementType.METHOD,ElementType.FIELD,ElementType.TYPE}) @Retention(RetentionPolicy.RUNTIME) public @interface RateLimitAspect { } 自定义切面类解析自定义注解 import com.alibaba.fastjson.JSON; import com.google.common.util.concurrent.RateLimiter; import lombok.extern.slf4j.Slf4j; import org.aspectj.lang.ProceedingJoinPoint; import org.aspectj.lang.annotation.Around; import org.aspectj.lang.annotation.Aspect; import org.aspectj.lang.annotation.Pointcut; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.context.annotation.Scope; import org.springframework.stereotype.Component; import javax.servlet.ServletOutputStream; import javax.servlet.http.HttpServletResponse; import java.io.IOException; /** * 自定义切面类解析注解 * * @author huang * @PACKAGE_NAME cn.huangsm.advance.ratelimiter.aop * @PROJECT_NAME advance-code * @date 2019/1/19 */ @Component @Aspect @Scope @Slf4j public class RateLimitAop { @Autowired private HttpServletResponse response; /** * 需要处理一个任务列表，但我们不希望每秒的任务提交超过五个 */ private RateLimiter rateLimiter = RateLimiter.create(5.0); @Pointcut(&quot;@annotation(cn.huangsm.advance.ratelimiter.RateLimitAspect)&quot;) public void rateLimiterPointCut() { } ; @Around(&quot;rateLimiterPointCut()&quot;) public Object around(ProceedingJoinPoint joinPoint) { /** * 得到令牌 */ Boolean flag = rateLimiter.tryAcquire(); Object obj = null; try { if (flag) { obj = joinPoint.proceed(); log.info(&quot;得到令牌!&quot;); } else { String result = JSON.toJSONString(&quot;抱歉，操作过于频繁，请稍等片刻在操作！&quot;); log.info(&quot;未获得令牌!&quot;); output(response, result); } } catch (Throwable e) { e.printStackTrace(); } System.out.println(&quot;是否得到令牌=&quot; + flag + &quot;,参数=&quot; + obj); return obj; } public void output(HttpServletResponse response, String msg) throws IOException { response.setContentType(&quot;application/json;charset=UTF-8&quot;); ServletOutputStream outputStream = null; try { outputStream = response.getOutputStream(); outputStream.write(msg.getBytes(&quot;UTF-8&quot;)); } catch (IOException e) { e.printStackTrace(); } finally { outputStream.flush(); outputStream.close(); } } } 测试Controller类 /** * 限流测试 * @author huang * @PACKAGE_NAME cn.huangsm.advance.ratelimiter * @PROJECT_NAME advance-code * @date 2019/1/19 */ @RestController public class TestController { @RateLimitAspect @GetMapping(&quot;/test&quot;) public String test(){ return &quot;nihao&quot;; } } 测试阶段 上文中的编码阶段算是结束了，总体来说就是自定义一个限流注解方便以后每次的使用，其实我这里做的还不够细致，如果可以的话可以自定义速率，本例中默认速率为5.0表示：想象下我们需要处理一个任务列表，但我们不希望每秒的任务提交超过五个。 话不多说开始测试首先打开JMeter并发测试工具类，定义好线程组，以及Http请求和结果树，在线程组和HTTP请求中输入必要的参数JMeter线程组参数HTTP请求参数启动测试可以看到有10个结果在结果树中分别查看10个HTTP请求的响应调用成功限流请求结果 结论不管重试多少次都是10次请求有4次被限制访问，6次成功访问. 因此我重新设置线程次的参数为20个线程，重新发送请求后得到结果为:6次成功，14次失败，得出结论美妙的认为提交不超过六个。对于为什么是五个而不是六个，在RateLimiter的github看下issue了解。]]></content>
      <categories>
        <category>RateLimiter</category>
      </categories>
      <tags>
        <tag>RateLimiter</tag>
        <tag>限流</tag>
        <tag>自定义注解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Docker部署maven私服]]></title>
    <url>%2F2019%2F01%2F18%2F%E4%BD%BF%E7%94%A8Docker%E9%83%A8%E7%BD%B2maven%E7%A7%81%E6%9C%8D%2F</url>
    <content type="text"><![CDATA[使用docker配置maven私服环境准备 安装docker环境使用docker -v查看服务器docker环境 下拉nexus3镜像使用命令，拉去最新版本的nexus镜像 docker pull sonatype/nexus3 启动镜像使用命令 docker run –rm -d –privileged=true -p 8088:8088 –name nexus -v /root/nexus-data:/var/nexus-data sonatype/nexus3 上面命令是指使用nexus3镜像创建并启动一个容器，然后指定暴露8081端口到对应主机的8081端口 将容器内部/var/nexus-data挂载到主机/root/nexus-data目录。 如果没有任何问题的话，Nexus应该是搭建成功了。 此时在浏览器输入：http://ip:8081即可看到以下页面：(ip为远程主机的ip地址) 修改nexus帐号密码 点击右上方的Sign in进行登录，初始账号密码为admin/admin123.请登录后修改密码如下图可以看到默认情况下Nexus会帮我们创建了几个仓库，仔细观察红色框住的地方，里面有几种仓库的类型，解释如下： proxy 远程仓库的代理，比如说nexus配置了一个central repository的proxy,当用户向这个proxy请求一个 artifact的时候，会现在本地查找，如果找不到，则会从远程仓库下载，然后返回给用户。 hosted 宿主仓库，用户可以把自己的一些仓库deploy到这个仓库中 group 仓库组，是nexus特有的概念，目的是将多个仓库整合，对用户暴露统一的地址，这样就不需要配置多个仓库地址。 下面我们仔细看一下里面的一些仓库。点击maven-central仓库:可以看到是一个proxy类型的仓库，他代理的远程仓库地址是https://repo1.maven.org/maven2/。后退，在进入maven-public查看:可以看到这是一个group类型的仓库，里面包含了maven-releases/maven-snapshots/maven-central仓库，意思是我们只需要在本地添加这个仓库，则可以依赖到上述3个仓库中的库了。]]></content>
      <categories>
        <category>maven私服</category>
      </categories>
      <tags>
        <tag>maven</tag>
        <tag>nexus</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ribbon+Feign扩展]]></title>
    <url>%2F2019%2F01%2F18%2FRibbon-Feign%E6%89%A9%E5%B1%95%2F</url>
    <content type="text"><![CDATA[Ribbon扩展切换Ribbon加载策略，ribbon默认加载策略为懒加载，修改为饥饿加载ribbon: eager-load: enabled: true # 多个用,分隔 clients: microservice-provider-user Feign配置自定义【通用配置】feign: client: config: default: connectTimeout: 5000 readTimeout: 5000 loggerLevel: basic 配置优先级如果你不小心又使用了Java代码配置Feign，同时又使用了配置属性配置Feign，那么使用配置属性的优先级更高。配置属性配置的方式将会覆盖Java代码配置。 如果你想修改代码配置方式的优先级，可使用如下属性：feign.client.default-to-properties=false 。 压缩一些场景下，我们可能需要对请求或响应进行压缩，此时可使用以下属性启用Feign的压缩功能。 feign.compression.request.enabled=true feign.compression.response.enabled=true 对于请求的压缩，Feign还提供了更为详细的设置，例如： feign.compression.request.enabled=true feign.compression.request.mime-types=text/xml,application/xml,application/json feign.compression.request.min-request-size=2048 其中，feign.compression.request.mime-types 用于支持的媒体类型列表，默认是text/xml、application/xml以及application/json。feign.compression.request.min-request-size 用于设置请求的最小阈值，默认是2048。]]></content>
      <categories>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[工作-人事管理项目开发]]></title>
    <url>%2F2019%2F01%2F18%2F%E5%B7%A5%E4%BD%9C-%E4%BA%BA%E4%BA%8B%E7%AE%A1%E7%90%86%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[概述&nbsp;&nbsp;&nbsp;&nbsp;这是我的第二份工作的第二个项目，相比于第一个安全响应项目。这个项目是针对公司内部使用的人事管理系统。主要包含数据的导入和导出，已经公司人员的管理和工资记录的管理，另一个就是这个系统基本上都是我来开发的。除了框架是公司的一个同事搭建的，因为考虑到后期该系统会调用第三方接口因此框架设计使用的和公司一样的架子，引入公司封装好的一些启动器，关闭eureka注册中心后我就开始工作了，说是架子是公司同事搭建的其实本质的技术还是我来进行选择的。 技术选型1SpringBoot+Mybatis-plus+Mybatis代码生成器+POI+SpringCloud+OpenFeign+swagger2+公司的一些工具类和异常处理等 开发前期&nbsp;&nbsp;&nbsp;&nbsp; 因为需求很简单这样我就需要先考虑系统的优化，考虑到并发小因此选择使用version字段来实现乐观锁，使用mybatis-plus的乐观锁插件来控制接口的幂等性，从而解决防止同一人操作同一条数据带来的脏数据问题。对于一些工资计算算法进行封装工具类，创建常用的策略模式,这些做完后基本上就可以开始开发了。 感受&nbsp;&nbsp;&nbsp;&nbsp;这是我第二次开发前后端分离的项目，关于前后端分离对我来说是便捷的，起码我不需要去考虑前后端数据的交互了，但是这对刚刚进入工作不久的我不是个好事，因此在空闲时间我还是需要去研究前端学习一些前端的框架，保持自己的竞争力。]]></content>
      <categories>
        <category>工作</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>工作日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[技术图谱]]></title>
    <url>%2F2019%2F01%2F17%2F%E6%8A%80%E6%9C%AF%E5%9B%BE%E8%B0%B1%2F</url>
    <content type="text"><![CDATA[这是我收集的一些技术图片，是我平时写的一下项目的架构，以及一些用到的流程图. Java技术图片 Java高级体系结构图 毕业设计框架图 异步化结构图]]></content>
      <categories>
        <category>技术图谱</category>
      </categories>
      <tags>
        <tag>technology_image</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[SB框架]SpringBoot中对多个配置文件中的属性进行提取的简易方法]]></title>
    <url>%2F2019%2F01%2F17%2FSB%E6%A1%86%E6%9E%B6-SpringBoot%E4%B8%AD%E5%AF%B9%E5%A4%9A%E4%B8%AA%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E4%B8%AD%E7%9A%84%E5%B1%9E%E6%80%A7%E8%BF%9B%E8%A1%8C%E6%8F%90%E5%8F%96%E7%9A%84%E7%AE%80%E6%98%93%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[SpringBoot中对多个配置文件中的属性进行提取的简易方法 我们要提取一下属性:首先创建一个GirlProperties类 12345678910111213141516171819202122232425262728293031323334package com.springboot.properties;import org.springframework.boot.context.properties.ConfigurationProperties;import org.springframework.stereotype.Component;/** * @BelongsProject: springbootideademo * @BelongsPackage: com.springboot.properties * @Author: HUANG * @CreateTime: 2018-11-15 21:46 * @PROJECT_NAME: springbootideademo */@Component@ConfigurationProperties(prefix = &quot;girl&quot;)public class GirlProperties &#123; private String cupSize; private Integer age; public String getCupSize() &#123; return cupSize; &#125; public void setCupSize(String cupSize) &#123; this.cupSize = cupSize; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125;&#125; 调用方式: 通过属性调用:]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
</search>
